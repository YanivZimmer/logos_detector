{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 1: Improved Logo Detector\n",
        "\n",
        "This notebook implements improvements to the baseline model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Check device (MPS for Mac M1)\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load baseline information\n",
        "with open('baseline_info.json', 'r') as f:\n",
        "    baseline_info = json.load(f)\n",
        "\n",
        "print(\"Baseline metrics:\")\n",
        "print(f\"  Val mAP50: {baseline_info['metrics']['val_map50']:.4f}\")\n",
        "print(f\"  Test mAP50: {baseline_info['metrics']['test_map50']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ultralytics if not already installed\n",
        "try:\n",
        "    from ultralytics import YOLO\n",
        "    print(\"Ultralytics already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing ultralytics...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([\"pip\", \"install\", \"ultralytics\"])\n",
        "    from ultralytics import YOLO\n",
        "\n",
        "# Load dataset config\n",
        "config_path = Path(baseline_info['dataset_config'])\n",
        "all_classes = baseline_info['classes']\n",
        "\n",
        "print(\"Initializing YOLO11\")\n",
        "#improved_model = YOLO('yolo11n.pt')  \n",
        "improved_model = YOLO('yolo11n.pt')  \n",
        "\n",
        "print(f\"Model initialized. Device: {device}\")\n",
        "print(f\"Number of classes: {len(all_classes)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training with Improvements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train improved model with enhanced settings\n",
        "print(\"Starting improved model training...\")\n",
        "print(f\"Dataset config: {config_path}\")\n",
        "\n",
        "improved_results = improved_model.train(\n",
        "    # Core Data & Device Settings\n",
        "    data=str(config_path),\n",
        "    epochs=40,             # Sufficient time for rare classes to converge\n",
        "    imgsz=640,\n",
        "    batch=8,               \n",
        "    device=str(device),\n",
        "    patience=15,           # Higher patience to allow minority classes to catch up\n",
        "    \n",
        "    # Loss Weights: Tuned for Identity over mere Localization\n",
        "    box=7.5,    # Increase weight of box loss (default is usually 7.5)\n",
        "    cls=0.5,    # Weight of classification loss (default 0.5)\n",
        "    dfl=1.5,    # Weight of distribution focal loss (default 1.5)\n",
        "    \n",
        "    # Augmentation: The \"Logo-Safe\" Suite\n",
        "    hsv_h=0.015,           # Restored for color robustness\n",
        "    hsv_s=0.7,\n",
        "    hsv_v=0.4,\n",
        "    degrees=3.0,           # Subtle rotation only\n",
        "    translate=0.1,         # Shift objects to learn position independence\n",
        "    scale=0.3,             # Reduced to prevent logos from becoming too tiny\n",
        "    mosaic=1.0,            # High density learning for small objects\n",
        "    copy_paste=0.3,        # CRITICAL: Helps balance SAP/FedEx by reusing instances\n",
        "    \n",
        "    # Explicitly Omitted/Disabled (The \"Risky\" ones for logos)\n",
        "    shear=0.0,             # Prevents distortion of circles/squares\n",
        "    perspective=0.0,       # Prevents unrealistic 3D warping\n",
        "    flipud=0.0,            # Logos should not be upside down\n",
        "    #fliplr=0.5,            # Remove Horizontal flips\n",
        "    mixup=0.0,             # Keep features sharp for small objects\n",
        "    \n",
        "    # Final Optimization\n",
        "    close_mosaic=10,       # Disable mosaic in last 10 epochs to sharpen edges\n",
        "    val=True,              # Ensure validation runs to monitor F1-score\n",
        "    plots=True             # Generate result plots for final analysis\n",
        ")\n",
        "print(\"\\nImproved training completed!\")\n",
        "print(f\"Results saved to: {improved_results.save_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the best improved model\n",
        "improved_best = YOLO(improved_results.save_dir / 'weights' / 'best.pt')\n",
        "\n",
        "# Evaluate on validation set\n",
        "print(\"Evaluating improved model on validation set...\")\n",
        "val_metrics_improved = improved_best.val(data=str(config_path), split='val')\n",
        "print(f\"\\nImproved Validation mAP50: {val_metrics_improved.box.map50:.4f}\")\n",
        "print(f\"Improved Validation mAP50-95: {val_metrics_improved.box.map:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\nEvaluating improved model on test set...\")\n",
        "test_metrics_improved = improved_best.val(data=str(config_path), split='test')\n",
        "print(f\"\\nImproved Test mAP50: {test_metrics_improved.box.map50:.4f}\")\n",
        "print(f\"Improved Test mAP50-95: {test_metrics_improved.box.map:.4f}\")\n",
        "\n",
        "# Save metrics\n",
        "improved_metrics = {\n",
        "    'val_map50': float(val_metrics_improved.box.map50),\n",
        "    'val_map50_95': float(val_metrics_improved.box.map),\n",
        "    'test_map50': float(test_metrics_improved.box.map50),\n",
        "    'test_map50_95': float(test_metrics_improved.box.map)\n",
        "}\n",
        "\n",
        "with open('improved_metrics.json', 'w') as f:\n",
        "    json.dump(improved_metrics, f, indent=2)\n",
        "\n",
        "print(\"\\nImproved metrics saved to improved_metrics.json\")\n",
        "\n",
        "# Compare with baseline\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON: Baseline vs Improved\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Metric':<25} {'Baseline':<15} {'Improved':<15} {'Improvement':<15}\")\n",
        "print(\"-\"*60)\n",
        "print(f\"{'Val mAP50':<25} {baseline_info['metrics']['val_map50']:<15.4f} {improved_metrics['val_map50']:<15.4f} {improved_metrics['val_map50'] - baseline_info['metrics']['val_map50']:+.4f}\")\n",
        "print(f\"{'Val mAP50-95':<25} {baseline_info['metrics']['val_map50_95']:<15.4f} {improved_metrics['val_map50_95']:<15.4f} {improved_metrics['val_map50_95'] - baseline_info['metrics']['val_map50_95']:+.4f}\")\n",
        "print(f\"{'Test mAP50':<25} {baseline_info['metrics']['test_map50']:<15.4f} {improved_metrics['test_map50']:<15.4f} {improved_metrics['test_map50'] - baseline_info['metrics']['test_map50']:+.4f}\")\n",
        "print(f\"{'Test mAP50-95':<25} {baseline_info['metrics']['test_map50_95']:<15.4f} {improved_metrics['test_map50_95']:<15.4f} {improved_metrics['test_map50_95'] - baseline_info['metrics']['test_map50_95']:+.4f}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Training Results - Improved Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training results\n",
        "results_csv = improved_results.save_dir / 'results.csv'\n",
        "if results_csv.exists():\n",
        "    import pandas as pd\n",
        "    results_df = pd.read_csv(results_csv)\n",
        "    \n",
        "    # Plot training and validation losses\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # Box loss\n",
        "    axes[0, 0].plot(results_df['epoch'], results_df['train/box_loss'], label='Train Box Loss', linewidth=2)\n",
        "    axes[0, 0].plot(results_df['epoch'], results_df['val/box_loss'], label='Val Box Loss', linewidth=2)\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Box Loss')\n",
        "    axes[0, 0].set_title('Improved Model: Box Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Classification loss\n",
        "    axes[1, 0].plot(results_df['epoch'], results_df['train/cls_loss'], label='Train Cls Loss', linewidth=2)\n",
        "    axes[1, 0].plot(results_df['epoch'], results_df['val/cls_loss'], label='Val Cls Loss', linewidth=2)\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Classification Loss')\n",
        "    axes[1, 0].set_title('Improved Model: Classification Loss')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # mAP50\n",
        "    axes[1, 1].plot(results_df['epoch'], results_df['metrics/mAP50(B)'], label='mAP50', linewidth=2, color='green')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('mAP50')\n",
        "    axes[1, 1].set_title('Improved Model: Validation mAP50')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('improved_training_curves.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Training curves saved to improved_training_curves.png\")\n",
        "    \n",
        "    # Compare training curves with baseline\n",
        "    baseline_csv = Path(baseline_info['model_path']).parent.parent / 'results.csv'\n",
        "    if baseline_csv.exists():\n",
        "        baseline_df = pd.read_csv(baseline_csv)\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "        \n",
        "        # Box loss comparison\n",
        "        axes[0, 0].plot(baseline_df['epoch'], baseline_df['val/box_loss'], \n",
        "                       label='Baseline Val Box Loss', linewidth=2, linestyle='--', alpha=0.7)\n",
        "        axes[0, 0].plot(results_df['epoch'], results_df['val/box_loss'], \n",
        "                       label='Improved Val Box Loss', linewidth=2)\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].set_ylabel('Validation Box Loss')\n",
        "        axes[0, 0].set_title('Box Loss Comparison')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # mAP50 comparison\n",
        "        axes[0, 1].plot(baseline_df['epoch'], baseline_df['metrics/mAP50(B)'], \n",
        "                        label='Baseline mAP50', linewidth=2, linestyle='--', alpha=0.7)\n",
        "        axes[0, 1].plot(results_df['epoch'], results_df['metrics/mAP50(B)'], \n",
        "                       label='Improved mAP50', linewidth=2)\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('mAP50')\n",
        "        axes[0, 1].set_title('mAP50 Comparison')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        \n",
        "        baseline_total = baseline_df['val/box_loss']  + baseline_df['val/cls_loss']\n",
        "        improved_total = results_df['val/box_loss']  + results_df['val/cls_loss']\n",
        "\n",
        "        axes[1, 0].plot(baseline_df['epoch'], baseline_total, \n",
        "                       label='Baseline Total Val Loss', linewidth=2, linestyle='--', alpha=0.7)\n",
        "        axes[1, 0].plot(results_df['epoch'], improved_total, \n",
        "                       label='Improved Total Val Loss', linewidth=2)\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].set_ylabel('Total Validation Loss')\n",
        "        axes[1, 0].set_title('Total Loss Comparison')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Metrics comparison bar chart\n",
        "        metrics_comparison = {\n",
        "            'Val mAP50': [baseline_info['metrics']['val_map50'], improved_metrics['val_map50']],\n",
        "            'Test mAP50': [baseline_info['metrics']['test_map50'], improved_metrics['test_map50']],\n",
        "            'Val mAP50-95': [baseline_info['metrics']['val_map50_95'], improved_metrics['val_map50_95']],\n",
        "            'Test mAP50-95': [baseline_info['metrics']['test_map50_95'], improved_metrics['test_map50_95']]\n",
        "        }\n",
        "        \n",
        "        x = np.arange(len(metrics_comparison))\n",
        "        width = 0.35\n",
        "        fig2, ax = plt.subplots(figsize=(12, 6))\n",
        "        baseline_vals = [v[0] for v in metrics_comparison.values()]\n",
        "        improved_vals = [v[1] for v in metrics_comparison.values()]\n",
        "        ax.bar(x - width/2, baseline_vals, width, label='Baseline', alpha=0.8)\n",
        "        ax.bar(x + width/2, improved_vals, width, label='Improved', alpha=0.8)\n",
        "        ax.set_ylabel('mAP Score')\n",
        "        ax.set_title('Model Performance Comparison')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(metrics_comparison.keys())\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3, axis='y')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"Comparison plots saved to model_comparison.png\")\n",
        "else:\n",
        "    print(\"Results CSV not found. Training may still be in progress.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_csv = improved_results.save_dir / 'results.csv'\n",
        "\n",
        "if results_csv.exists():\n",
        "    import pandas as pd\n",
        "    results_df = pd.read_csv(results_csv)\n",
        "    \n",
        "    # Plot training and validation losses\n",
        "    #fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fix, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "    # Box loss\n",
        "    axes[0, 0].plot(results_df['epoch'], results_df['train/box_loss'], label='Train Box Loss', linewidth=2)\n",
        "    axes[0, 0].plot(results_df['epoch'], results_df['val/box_loss'], label='Val Box Loss', linewidth=2)\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Box Loss')\n",
        "    axes[0, 0].set_title('Box Loss: Training vs Validation')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "\n",
        "    # mAP50-95\n",
        "    axes[1, 1].plot(results_df['epoch'], results_df['metrics/mAP50-95(B)'], label='mAP50-95', linewidth=2, color='green')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('mAP50-95')\n",
        "    axes[1, 1].set_title('Validation mAP50-95 Over Time')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Classification loss\n",
        "    axes[0, 1].plot(results_df['epoch'], results_df['train/cls_loss'], label='Train Cls Loss', linewidth=2)\n",
        "    axes[0, 1].plot(results_df['epoch'], results_df['val/cls_loss'], label='Val Cls Loss', linewidth=2)\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Classification Loss')\n",
        "    axes[0, 1].set_title('Classification Loss: Training vs Validation')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    #train/dfl_loss\n",
        "    axes[0, 2].plot(results_df['epoch'], results_df['train/dfl_loss'], label='Train dfl Loss', linewidth=2)\n",
        "    axes[0, 2].plot(results_df['epoch'], results_df['val/dfl_loss'], label='Val dfl Loss', linewidth=2)\n",
        "    axes[0, 2].set_xlabel('Epoch')\n",
        "    axes[0, 2].set_ylabel('dfl Loss')\n",
        "    axes[0, 2].set_title('dfl Loss: Training vs Validation')\n",
        "    axes[0, 2].legend()\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "    \n",
        "    # mAP50\n",
        "    axes[1, 0].plot(results_df['epoch'], results_df['metrics/mAP50(B)'], label='mAP50', linewidth=2, color='green')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('mAP50')\n",
        "    axes[1, 0].set_title('Validation mAP50 Over Time')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "\n",
        "    results_df['metrics/F1(B)'] = 2 * (results_df['metrics/precision(B)'] * results_df['metrics/recall(B)']) / \\\n",
        "                                 (results_df['metrics/precision(B)'] + results_df['metrics/recall(B)'] + 1e-6)\n",
        "\n",
        "    # Add this to your plotting loop or as a new subplot\n",
        "    axes[1, 2].plot(results_df['epoch'], results_df['metrics/F1(B)'], label='F1 Score', color='orange', linewidth=2)\n",
        "    axes[1, 2].set_title('Model Reliability (F1 Score)')\n",
        "    axes[1, 2].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('improved0_training_curves.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Training curves saved to improved0_training_curves.png\")\n",
        "else:\n",
        "    print(\"Results CSV not found. Training may still be in progress.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run predictions on test images\n",
        "YOLO_DATASET = Path(\"yolo_dataset\")\n",
        "test_image_files = list((YOLO_DATASET / 'test' / 'images').glob(\"*.jpg\"))[:12]\n",
        "\n",
        "# Get predictions\n",
        "predictions = improved_best.predict(\n",
        "    source=[str(f) for f in test_image_files],\n",
        "    conf=0.25,\n",
        "    iou=0.45,\n",
        "    save=False,\n",
        "    show=False\n",
        ")\n",
        "\n",
        "# Visualize predictions\n",
        "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, (img_file, pred) in enumerate(zip(test_image_files, predictions)):\n",
        "    # Load original image\n",
        "    img = Image.open(img_file)\n",
        "    axes[idx].imshow(img)\n",
        "    axes[idx].axis('off')\n",
        "    axes[idx].set_title(f\"{img_file.name[:40]}...\", fontsize=8)\n",
        "    \n",
        "    # Draw predictions\n",
        "    if pred.boxes is not None and len(pred.boxes) > 0:\n",
        "        boxes = pred.boxes.xyxy.cpu().numpy()\n",
        "        confidences = pred.boxes.conf.cpu().numpy()\n",
        "        classes = pred.boxes.cls.cpu().numpy().astype(int)\n",
        "        \n",
        "        for box, conf, cls in zip(boxes, confidences, classes):\n",
        "            x1, y1, x2, y2 = box\n",
        "            class_name = all_classes[cls]\n",
        "            \n",
        "            # Draw bounding box\n",
        "            from matplotlib.patches import Rectangle\n",
        "            rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, \n",
        "                           edgecolor='cyan', facecolor='none')\n",
        "            axes[idx].add_patch(rect)\n",
        "            axes[idx].text(x1, y1-5, f\"{class_name} {conf:.2f}\", \n",
        "                          color='cyan', fontsize=7,\n",
        "                          bbox=dict(boxstyle='round,pad=0.3', facecolor='black', alpha=0.7))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('improved_test_predictions.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Improved test predictions saved to improved_test_predictions.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save improved model info for Section 2\n",
        "improved_info = {\n",
        "    'model_path': str(improved_results.save_dir / 'weights' / 'best.pt'),\n",
        "    'classes': all_classes,\n",
        "    'num_classes': len(all_classes),\n",
        "    'metrics': improved_metrics,\n",
        "    'dataset_config': str(config_path),\n",
        "    'baseline_metrics': baseline_info['metrics']\n",
        "}\n",
        "\n",
        "with open('improved_info.json', 'w') as f:\n",
        "    json.dump(improved_info, f, indent=2)\n",
        "\n",
        "print(\"\\nImproved model information saved to improved_info.json\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"IMPROVED MODEL COMPLETE\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Baseline Test mAP50: {baseline_info['metrics']['test_map50']:.4f}\")\n",
        "print(f\"Improved Test mAP50: {improved_metrics['test_map50']:.4f}\")\n",
        "print(f\"Improvement: {improved_metrics['test_map50'] - baseline_info['metrics']['test_map50']:+.4f} ({((improved_metrics['test_map50'] / baseline_info['metrics']['test_map50'] - 1) * 100):+.2f}%)\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
