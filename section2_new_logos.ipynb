{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 2: New Logo Detection\n",
        "\n",
        "This notebook addresses the challenge of detecting new logos that were not in the original training set.\n",
        "\n",
        "## Problem:\n",
        "- Customer provides an image with new logos (e.g., Adidas, NBA, Miami Heat)\n",
        "- These logos may not be in the original training classes\n",
        "- Need to detect and localize these new logos\n",
        "\n",
        "## Approach:\n",
        "1. **Few-shot Learning**: Use the pre-trained model as a feature extractor\n",
        "2. **Template Matching**: Extract logo templates from the provided image\n",
        "3. **Fine-tuning**: Fine-tune the model on new logo classes\n",
        "4. **Hybrid Approach**: Combine template matching with object detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import torch\n",
        "import cv2\n",
        "from collections import Counter\n",
        "import yaml\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Check device (MPS for Mac M1)\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Section 2 Data and Analyze\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Section 2 data\n",
        "SECTION2_ROOT = Path(\"raw_data/logos-dataset-section-2\")\n",
        "section2_image = SECTION2_ROOT / \"Adidas_38.jpg\"\n",
        "section2_label = SECTION2_ROOT / \"Adidas_38.txt\"\n",
        "\n",
        "# Load and visualize the image\n",
        "img = Image.open(section2_image)\n",
        "print(f\"Image size: {img.size}\")\n",
        "print(f\"Image mode: {img.mode}\")\n",
        "\n",
        "# Load annotations\n",
        "with open(section2_label, 'r') as f:\n",
        "    annotations = f.readlines()\n",
        "\n",
        "print(f\"\\nNumber of annotations: {len(annotations)}\")\n",
        "print(\"\\nAnnotations:\")\n",
        "for ann in annotations:\n",
        "    print(f\"  {ann.strip()}\")\n",
        "\n",
        "# Extract new classes\n",
        "new_classes = []\n",
        "for ann in annotations:\n",
        "    parts = ann.strip().split()\n",
        "    if len(parts) >= 5:\n",
        "        class_name = parts[0]\n",
        "        if class_name not in new_classes:\n",
        "            new_classes.append(class_name)\n",
        "\n",
        "print(f\"\\nNew classes found: {new_classes}\")\n",
        "\n",
        "# Visualize the image with ground truth bounding boxes\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "ax.imshow(img)\n",
        "ax.axis('off')\n",
        "ax.set_title('Section 2 Image with Ground Truth Annotations', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Draw bounding boxes\n",
        "img_width, img_height = img.size\n",
        "for ann in annotations:\n",
        "    parts = ann.strip().split()\n",
        "    if len(parts) >= 5:\n",
        "        class_name = parts[0]\n",
        "        x_center, y_center, width, height = [float(x) for x in parts[1:5]]\n",
        "        \n",
        "        # Convert normalized coordinates to pixel coordinates\n",
        "        x = (x_center - width/2) * img_width\n",
        "        y = (y_center - height/2) * img_height\n",
        "        w = width * img_width\n",
        "        h = height * img_height\n",
        "        \n",
        "        # Draw bounding box\n",
        "        from matplotlib.patches import Rectangle\n",
        "        rect = Rectangle((x, y), w, h, linewidth=3, \n",
        "                        edgecolor='red', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x, y-10, class_name, color='red', fontsize=12, fontweight='bold',\n",
        "               bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('section2_ground_truth.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nGround truth visualization saved to section2_ground_truth.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Pre-trained Model from Section 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load improved model info from Section 1\n",
        "try:\n",
        "    with open('improved_info.json', 'r') as f:\n",
        "        improved_info = json.load(f)\n",
        "    print(\"Loaded improved model info from Section 1\")\n",
        "    print(f\"Original classes: {len(improved_info['classes'])}\")\n",
        "except FileNotFoundError:\n",
        "    # Fallback to baseline if improved not available\n",
        "    with open('baseline_info.json', 'r') as f:\n",
        "        improved_info = json.load(f)\n",
        "    print(\"Loaded baseline model info from Section 1 (improved not found)\")\n",
        "\n",
        "original_classes = improved_info['classes']\n",
        "print(f\"\\nOriginal classes (first 10): {original_classes[:10]}...\")\n",
        "\n",
        "# Check if new classes are in original classes\n",
        "print(f\"\\nNew classes from Section 2: {new_classes}\")\n",
        "overlap = [cls for cls in new_classes if cls in original_classes]\n",
        "new_only = [cls for cls in new_classes if cls not in original_classes]\n",
        "\n",
        "print(f\"\\nClasses already in model: {overlap}\")\n",
        "print(f\"Truly new classes: {new_only}\")\n",
        "\n",
        "# Install ultralytics\n",
        "try:\n",
        "    from ultralytics import YOLO\n",
        "    print(\"\\nUltralytics already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing ultralytics...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([\"pip\", \"install\", \"ultralytics\"])\n",
        "    from ultralytics import YOLO\n",
        "\n",
        "# Load the pre-trained model\n",
        "pretrained_model_path = improved_info['model_path']\n",
        "print(f\"\\nLoading pre-trained model from: {pretrained_model_path}\")\n",
        "pretrained_model = YOLO(pretrained_model_path)\n",
        "print(\"Pre-trained model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Approach 1: Test Pre-trained Model on New Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the pre-trained model on the new image\n",
        "print(\"Testing pre-trained model on Section 2 image...\")\n",
        "predictions = pretrained_model.predict(\n",
        "    source=str(section2_image),\n",
        "    conf=0.25,\n",
        "    iou=0.45,\n",
        "    save=False,\n",
        "    show=False\n",
        ")\n",
        "\n",
        "pred = predictions[0]\n",
        "\n",
        "# Visualize predictions\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "ax.imshow(img)\n",
        "ax.axis('off')\n",
        "ax.set_title('Pre-trained Model Predictions on Section 2 Image', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Draw predictions\n",
        "if pred.boxes is not None and len(pred.boxes) > 0:\n",
        "    boxes = pred.boxes.xyxy.cpu().numpy()\n",
        "    confidences = pred.boxes.conf.cpu().numpy()\n",
        "    classes = pred.boxes.cls.cpu().numpy().astype(int)\n",
        "    \n",
        "    print(f\"\\nDetected {len(boxes)} objects:\")\n",
        "    for box, conf, cls in zip(boxes, confidences, classes):\n",
        "        x1, y1, x2, y2 = box\n",
        "        if cls < len(original_classes):\n",
        "            class_name = original_classes[cls]\n",
        "            print(f\"  {class_name}: {conf:.3f}\")\n",
        "            \n",
        "            # Draw bounding box\n",
        "            from matplotlib.patches import Rectangle\n",
        "            rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, \n",
        "                           edgecolor='lime', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            ax.text(x1, y1-5, f\"{class_name} {conf:.2f}\", \n",
        "                   color='lime', fontsize=10, fontweight='bold',\n",
        "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='black', alpha=0.7))\n",
        "else:\n",
        "    print(\"\\nNo objects detected by pre-trained model\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('section2_pretrained_predictions.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nPre-trained predictions saved to section2_pretrained_predictions.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Approach 2: Fine-tune Model with New Classes\n",
        "\n",
        "Since the new logos may not be in the original training set, we'll:\n",
        "1. Create a small dataset from the provided image\n",
        "2. Add new classes to the model\n",
        "3. Fine-tune the model on the new data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dataset for fine-tuning with new classes\n",
        "# We'll use the Section 2 image as training data\n",
        "# In a real scenario, we'd have more images, but we'll work with what we have\n",
        "\n",
        "SECTION2_DATASET = Path(\"section2_dataset\")\n",
        "SECTION2_DATASET.mkdir(exist_ok=True)\n",
        "\n",
        "# Create directories\n",
        "for split in ['train', 'val']:\n",
        "    (SECTION2_DATASET / split / 'images').mkdir(parents=True, exist_ok=True)\n",
        "    (SECTION2_DATASET / split / 'labels').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Copy image to train and val (we'll use the same image for both in this case)\n",
        "# In practice, you'd have more images\n",
        "shutil.copy(section2_image, SECTION2_DATASET / 'train' / 'images' / section2_image.name)\n",
        "shutil.copy(section2_image, SECTION2_DATASET / 'val' / 'images' / section2_image.name)\n",
        "\n",
        "# Create YOLO format labels with new classes\n",
        "# Combine original classes with new classes\n",
        "all_classes_combined = original_classes + [cls for cls in new_classes if cls not in original_classes]\n",
        "\n",
        "print(f\"Total classes: {len(all_classes_combined)}\")\n",
        "print(f\"Original: {len(original_classes)}, New: {len(new_only)}\")\n",
        "\n",
        "# Convert annotations to YOLO format\n",
        "def convert_to_yolo_format(label_file, output_file, class_mapping):\n",
        "    \"\"\"Convert annotation file to YOLO format with class indices\"\"\"\n",
        "    with open(label_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    with open(output_file, 'w') as f:\n",
        "        for line in lines:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) >= 5:\n",
        "                class_name = parts[0]\n",
        "                if class_name in class_mapping:\n",
        "                    class_idx = class_mapping[class_name]\n",
        "                    f.write(f\"{class_idx} {' '.join(parts[1:5])}\\n\")\n",
        "\n",
        "# Create class mapping\n",
        "class_mapping = {cls: idx for idx, cls in enumerate(all_classes_combined)}\n",
        "\n",
        "# Convert labels\n",
        "convert_to_yolo_format(\n",
        "    section2_label,\n",
        "    SECTION2_DATASET / 'train' / 'labels' / (section2_image.stem + '.txt'),\n",
        "    class_mapping\n",
        ")\n",
        "convert_to_yolo_format(\n",
        "    section2_label,\n",
        "    SECTION2_DATASET / 'val' / 'labels' / (section2_image.stem + '.txt'),\n",
        "    class_mapping\n",
        ")\n",
        "\n",
        "print(\"\\nSection 2 dataset created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset config for Section 2\n",
        "section2_config = {\n",
        "    'path': str(SECTION2_DATASET.absolute()),\n",
        "    'train': 'train/images',\n",
        "    'val': 'val/images',\n",
        "    'nc': len(all_classes_combined),\n",
        "    'names': all_classes_combined\n",
        "}\n",
        "\n",
        "section2_config_path = SECTION2_DATASET / 'dataset.yaml'\n",
        "with open(section2_config_path, 'w') as f:\n",
        "    yaml.dump(section2_config, f, default_flow_style=False)\n",
        "\n",
        "print(f\"Section 2 dataset config saved to: {section2_config_path}\")\n",
        "\n",
        "# Strategy: Fine-tune the model with new classes\n",
        "# We'll use transfer learning - start from pre-trained weights but allow the model\n",
        "# to learn new classes\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINE-TUNING STRATEGY\")\n",
        "print(\"=\"*60)\n",
        "print(\"1. Load pre-trained model from Section 1\")\n",
        "print(\"2. Extend model to include new classes\")\n",
        "print(\"3. Fine-tune on Section 2 data with low learning rate\")\n",
        "print(\"4. This allows the model to detect both old and new logos\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tune the model with new classes\n",
        "# We need to modify the model to support the new number of classes\n",
        "print(\"Fine-tuning model with new classes...\")\n",
        "\n",
        "# Create a new model with extended classes\n",
        "# YOLO will automatically handle class extension if we provide a model with different num_classes\n",
        "# We'll fine-tune with a very low learning rate to preserve existing knowledge\n",
        "\n",
        "finetuned_model = YOLO(pretrained_model_path)\n",
        "\n",
        "# Fine-tune with new classes\n",
        "# Use a very low learning rate to preserve pre-trained knowledge\n",
        "finetune_results = finetuned_model.train(\n",
        "    data=str(section2_config_path),\n",
        "    epochs=50,\n",
        "    imgsz=640,\n",
        "    batch=4,  # Small batch for small dataset\n",
        "    device=str(device),\n",
        "    project='runs/detect',\n",
        "    name='section2_finetuned',\n",
        "    exist_ok=True,\n",
        "    patience=10,\n",
        "    save=True,\n",
        "    plots=True,\n",
        "    verbose=True,\n",
        "    # Low learning rate for fine-tuning\n",
        "    lr0=0.001,  # 10x lower than normal training\n",
        "    lrf=0.01,\n",
        "    # Data augmentation (but be careful with small dataset)\n",
        "    hsv_h=0.01,\n",
        "    hsv_s=0.5,\n",
        "    hsv_v=0.3,\n",
        "    degrees=5,\n",
        "    translate=0.05,\n",
        "    scale=0.3,\n",
        "    flipud=0.0,\n",
        "    fliplr=0.5,\n",
        "    mosaic=0.5,  # Reduced mosaic for small dataset\n",
        "    mixup=0.0,  # No mixup for very small dataset\n",
        "    copy_paste=0.0,  # No copy-paste for very small dataset\n",
        ")\n",
        "\n",
        "print(\"\\nFine-tuning completed!\")\n",
        "print(f\"Results saved to: {finetune_results.save_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the fine-tuned model\n",
        "finetuned_best = YOLO(finetune_results.save_dir / 'weights' / 'best.pt')\n",
        "\n",
        "# Test on the Section 2 image\n",
        "print(\"Testing fine-tuned model on Section 2 image...\")\n",
        "predictions_finetuned = finetuned_best.predict(\n",
        "    source=str(section2_image),\n",
        "    conf=0.25,\n",
        "    iou=0.45,\n",
        "    save=False,\n",
        "    show=False\n",
        ")\n",
        "\n",
        "pred_ft = predictions_finetuned[0]\n",
        "\n",
        "# Visualize fine-tuned predictions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "# Ground truth\n",
        "axes[0].imshow(img)\n",
        "axes[0].axis('off')\n",
        "axes[0].set_title('Ground Truth', fontsize=14, fontweight='bold')\n",
        "\n",
        "img_width, img_height = img.size\n",
        "for ann in annotations:\n",
        "    parts = ann.strip().split()\n",
        "    if len(parts) >= 5:\n",
        "        class_name = parts[0]\n",
        "        x_center, y_center, width, height = [float(x) for x in parts[1:5]]\n",
        "        x = (x_center - width/2) * img_width\n",
        "        y = (y_center - height/2) * img_height\n",
        "        w = width * img_width\n",
        "        h = height * img_height\n",
        "        from matplotlib.patches import Rectangle\n",
        "        rect = Rectangle((x, y), w, h, linewidth=3, edgecolor='red', facecolor='none')\n",
        "        axes[0].add_patch(rect)\n",
        "        axes[0].text(x, y-10, class_name, color='red', fontsize=10, fontweight='bold',\n",
        "                    bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.8))\n",
        "\n",
        "# Fine-tuned predictions\n",
        "axes[1].imshow(img)\n",
        "axes[1].axis('off')\n",
        "axes[1].set_title('Fine-tuned Model Predictions', fontsize=14, fontweight='bold')\n",
        "\n",
        "if pred_ft.boxes is not None and len(pred_ft.boxes) > 0:\n",
        "    boxes = pred_ft.boxes.xyxy.cpu().numpy()\n",
        "    confidences = pred_ft.boxes.conf.cpu().numpy()\n",
        "    classes = pred_ft.boxes.cls.cpu().numpy().astype(int)\n",
        "    \n",
        "    print(f\"\\nFine-tuned model detected {len(boxes)} objects:\")\n",
        "    for box, conf, cls in zip(boxes, confidences, classes):\n",
        "        x1, y1, x2, y2 = box\n",
        "        if cls < len(all_classes_combined):\n",
        "            class_name = all_classes_combined[cls]\n",
        "            print(f\"  {class_name}: {conf:.3f}\")\n",
        "            \n",
        "            from matplotlib.patches import Rectangle\n",
        "            rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, \n",
        "                           edgecolor='cyan', facecolor='none')\n",
        "            axes[1].add_patch(rect)\n",
        "            axes[1].text(x1, y1-5, f\"{class_name} {conf:.2f}\", \n",
        "                       color='cyan', fontsize=10, fontweight='bold',\n",
        "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='black', alpha=0.7))\n",
        "else:\n",
        "    print(\"\\nNo objects detected by fine-tuned model\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('section2_finetuned_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nComparison saved to section2_finetuned_comparison.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Alternative Approach: Template Matching + Object Detection\n",
        "\n",
        "For scenarios with very limited data, we can use a hybrid approach:\n",
        "1. Extract logo templates from the provided image\n",
        "2. Use template matching for detection\n",
        "3. Combine with object detection for better accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract logo templates from the annotated image\n",
        "def extract_logo_templates(image_path, label_path, output_dir):\n",
        "    \"\"\"Extract logo regions as templates for template matching\"\"\"\n",
        "    img = cv2.imread(str(image_path))\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    h, w = img.shape[:2]\n",
        "    \n",
        "    templates = []\n",
        "    \n",
        "    with open(label_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) >= 5:\n",
        "                class_name = parts[0]\n",
        "                x_center, y_center, width, height = [float(x) for x in parts[1:5]]\n",
        "                \n",
        "                # Convert to pixel coordinates\n",
        "                x1 = int((x_center - width/2) * w)\n",
        "                y1 = int((y_center - height/2) * h)\n",
        "                x2 = int((x_center + width/2) * w)\n",
        "                y2 = int((y_center + height/2) * h)\n",
        "                \n",
        "                # Extract template with padding\n",
        "                padding = 5\n",
        "                x1 = max(0, x1 - padding)\n",
        "                y1 = max(0, y1 - padding)\n",
        "                x2 = min(w, x2 + padding)\n",
        "                y2 = min(h, y2 + padding)\n",
        "                \n",
        "                template = img_rgb[y1:y2, x1:x2]\n",
        "                templates.append({\n",
        "                    'class': class_name,\n",
        "                    'template': template,\n",
        "                    'bbox': (x1, y1, x2, y2)\n",
        "                })\n",
        "    \n",
        "    return templates\n",
        "\n",
        "# Extract templates\n",
        "templates = extract_logo_templates(section2_image, section2_label, None)\n",
        "\n",
        "print(f\"Extracted {len(templates)} logo templates\")\n",
        "\n",
        "# Visualize extracted templates\n",
        "fig, axes = plt.subplots(1, len(templates), figsize=(4*len(templates), 4))\n",
        "if len(templates) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for idx, template_info in enumerate(templates):\n",
        "    axes[idx].imshow(template_info['template'])\n",
        "    axes[idx].axis('off')\n",
        "    axes[idx].set_title(f\"{template_info['class']}\\n{template_info['template'].shape[:2]}\", \n",
        "                       fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('section2_templates.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTemplates saved to section2_templates.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Template matching function\n",
        "def template_matching(image, templates, threshold=0.6):\n",
        "    \"\"\"Perform template matching on image\"\"\"\n",
        "    img_gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
        "    detections = []\n",
        "    \n",
        "    for template_info in templates:\n",
        "        template = template_info['template']\n",
        "        template_gray = cv2.cvtColor(template, cv2.COLOR_RGB2GRAY)\n",
        "        \n",
        "        # Multi-scale template matching\n",
        "        scales = [0.5, 0.75, 1.0, 1.25, 1.5]\n",
        "        best_match_val = 0\n",
        "        best_match_loc = None\n",
        "        best_scale = 1.0\n",
        "        \n",
        "        for scale in scales:\n",
        "            if scale != 1.0:\n",
        "                h, w = template_gray.shape[:2]\n",
        "                scaled_template = cv2.resize(template_gray, (int(w*scale), int(h*scale)))\n",
        "            else:\n",
        "                scaled_template = template_gray\n",
        "            \n",
        "            if scaled_template.shape[0] > img_gray.shape[0] or scaled_template.shape[1] > img_gray.shape[1]:\n",
        "                continue\n",
        "                \n",
        "            result = cv2.matchTemplate(img_gray, scaled_template, cv2.TM_CCOEFF_NORMED)\n",
        "            min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
        "            \n",
        "            if max_val > best_match_val:\n",
        "                best_match_val = max_val\n",
        "                best_match_loc = max_loc\n",
        "                best_scale = scale\n",
        "        \n",
        "        if best_match_val >= threshold and best_match_loc is not None:\n",
        "            h, w = template_gray.shape[:2]\n",
        "            x1 = best_match_loc[0]\n",
        "            y1 = best_match_loc[1]\n",
        "            x2 = x1 + int(w * best_scale)\n",
        "            y2 = y1 + int(h * best_scale)\n",
        "            \n",
        "            detections.append({\n",
        "                'class': template_info['class'],\n",
        "                'bbox': (x1, y1, x2, y2),\n",
        "                'confidence': float(best_match_val)\n",
        "            })\n",
        "    \n",
        "    return detections\n",
        "\n",
        "# Test template matching\n",
        "img_array = np.array(img)\n",
        "template_detections = template_matching(img, templates, threshold=0.5)\n",
        "\n",
        "print(f\"Template matching found {len(template_detections)} detections:\")\n",
        "for det in template_detections:\n",
        "    print(f\"  {det['class']}: {det['confidence']:.3f} at {det['bbox']}\")\n",
        "\n",
        "# Visualize template matching results\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "ax.imshow(img)\n",
        "ax.axis('off')\n",
        "ax.set_title('Template Matching Results', fontsize=14, fontweight='bold')\n",
        "\n",
        "for det in template_detections:\n",
        "    x1, y1, x2, y2 = det['bbox']\n",
        "    from matplotlib.patches import Rectangle\n",
        "    rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, \n",
        "                    edgecolor='magenta', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(x1, y1-5, f\"{det['class']} {det['confidence']:.2f}\", \n",
        "           color='magenta', fontsize=10, fontweight='bold',\n",
        "           bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('section2_template_matching.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTemplate matching results saved to section2_template_matching.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary and Recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Section 2 results\n",
        "section2_results = {\n",
        "    'new_classes': new_classes,\n",
        "    'new_only_classes': new_only,\n",
        "    'all_classes_combined': all_classes_combined,\n",
        "    'finetuned_model_path': str(finetune_results.save_dir / 'weights' / 'best.pt'),\n",
        "    'template_matching_detections': len(template_detections),\n",
        "    'finetuned_detections': len(pred_ft.boxes) if pred_ft.boxes is not None else 0\n",
        "}\n",
        "\n",
        "with open('section2_results.json', 'w') as f:\n",
        "    json.dump(section2_results, f, indent=2)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"SECTION 2 SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"New classes detected: {new_classes}\")\n",
        "print(f\"Truly new classes (not in original): {new_only}\")\n",
        "print(f\"\\nApproaches tested:\")\n",
        "print(f\"  1. Pre-trained model: May detect similar logos\")\n",
        "print(f\"  2. Fine-tuning: Extends model to new classes\")\n",
        "print(f\"  3. Template matching: Works for exact logo matches\")\n",
        "print(f\"\\nRecommendations:\")\n",
        "print(f\"  - For production: Use fine-tuned model with more training data\")\n",
        "print(f\"  - For quick prototyping: Use template matching\")\n",
        "print(f\"  - Hybrid approach: Combine both methods\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nSection 2 results saved to section2_results.json\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
